---
title: "[nlp_paper] Seq2Seq Paper Review"
toc: true
use_math: true
categories:
  - paper
tags:
  - [keras, tensorflow, paper, model]
date: 2023-01-22
last_modified_at: 2023-01-22
sitemap:
  changefreq: daily
  priority: 1.0
---

DSL 논문 스터디 6기 손예진님이 발제하신 내용을 기반으로 작성했습니다.

## <span style = "color : blue"> Introduction </span>

**Seq2seq** neural network model encodes an input sequence, converts it as a fixed-length vector representation, and decodes it to produce an output sequence. <br> <br>
Seq2seq is commonly used to translate **sequence data** such as <br>
*Machine Translation, Speech Recognition,* and *QA.*

### <span style = "color : skyblue"> Model Architecture </span>

<img src = '/assets/images/paper/2.png' width = '300'> <br>

**Seq2seq** model is consisted of two major LSTM architectures: <br>

> **Encoder Architecture** <br>
> **Decoder Architecture**

**Encoder cell** inputs tokenized sequence data and summerizes it into one single hidden-state numerical vector (the **context vector**) <br>
<br>
**Decoder cell** receives the context vector and predicts the probability of $y_t$ given the value of the hidden LSTM cell $t-1$ and input vector $t$ until the end of sentence **\<EOS>** token.

## <span style = "color : blue"> Paper Review </span>

<img src = '/assets/images/paper/1.png' width = '600'> <br>

To summarize, **Seq2seq model** utilizes depth-4 lstm model. Each layer contains over 1,000 cells and 1,000 dimensional word embeddings. <br> <br>

Its **input vocab** is 160,000 sized, and **output vocab** is 80,000 sized vectors. <br> <br>

The model used a **naive softmax** onto each 80,000 vocabs on every outputs. <br ><br>

### <span style = "color : skyblue"> Reversed Source Sentences </span>

<img src = '/assets/images/paper/3.png' width = '600'> <br>

Seq2seq model also experiemnted an **reversed** **src** (source) sentences. When the source sentences are reversed and the **tgt** (target) sentences remain the same, the distance between the initial src token and the tgt token become closer.
<br> <br>
This has a significan impact since the Seq2seq model decodes data based on **sequential data** hidden state $t-1$ and input vector $t$. Since the prediction accuracy of the initial vectors improved, the overall performance also gained more accuracy.

<img src = '/assets/images/paper/4.png' width = '600'> <br>

### <span style = "color : skyblue"> Experimental Results </span>

The **experimental Results** are summarized as followed.
<br> <br>
The **BLEU score** evaluates the quality of text generated by machine translation systems. It compares the generated text to human translated text by calculating the **n-gram overlap.**
<br><br>
**Seq2seq** LSTM model shows *remarkable improvement* from traditional machine translation models although it is way more **cheaper** than these SMTs.

### <span style = "color : skyblue"> Limitations & Improvements </span>

**Seq2seq** model has **two** major problems.
<br><br>
> 1. The Encoder cell summarizes all its information into a single context vector. It results in significant **information loss.** <br>
> 2. LSTM is a RNN model. So, it has its chronic problem: **vanishing gradient**.

<br>

The first problem of Seqseq model, which comes from summerizing information into a single vector, is improved on the next model : **attention + Seq2seq** which will be covered at the **next post.**
